#!/bin/zsh
# We need to check if we're removing the snapshot associated with the latest
# successful backup. If yes, it's also the only successful backup, which is
# another reason we can't remove it. If not, it cannot be the only
# successful backup.

DESTROYLOCK=/var/lock/korn.zfsbackup.zfs.destroy.lock
LOCKTIMEOUT=3600

LOG_LEVEL=warning
DEFAULT_FUNCTIONS=/usr/local/share/zfsbackup/functions.zsh

. $DEFAULT_FUNCTIONS
[[ -r /etc/zfsbackup/server.conf ]] && . /etc/zfsbackup/server.conf
[[ -r /etc/zfsbackup/${0:t}.conf ]] && . /etc/zfsbackup/${0:t}.conf
[[ -n $USER_FUNCTIONS ]] && [[ -r $USER_FUNCTIONS ]] && . $USER_FUNCTIONS	# allow user functions to override default functions

me="${0:t}${1:+:$1}"

zmodload zsh/system
zmodload zsh/datetime

[[ -r /etc/zfsbackup/server.conf ]] && . /etc/zfsbackup/server.conf

# Exit silently if the snapshot doesn't exist; it was either removed
# manually or by a cronjob, and there is nothing useful for this script to
# do. Printing an error message would just result in mail being sent.
#
# If we run into errors later, we'll check again if it's because the
# snapshot got removed in the meantime.
zfs get -Hp -o value name "$1" 2>/dev/null >/dev/null || exit 0

# Obtain an exclusive lock to avoid race conditions arising from several concurrent instances of this script
# trying to remove different snapshots, making the decision whether we're about to remove the snapshot of the
# last successful backup unpredictable. It would be better to use per-fs locks, but I need to think about the
# implications for recursive snapshots and their removal.
: >>$DESTROYLOCK
if ! zsystem flock -f lockfd -t $LOCKTIMEOUT $DESTROYLOCK; then
	die "unable to obtain lock on $DESTROYLOCK; not destroying $1."
fi

expires=$(zfs get -Hp -o value ${PROPPREFIX}:expires "$1" 2>/dev/null) || {
	# Again, exit silently if the snapshot doesn't exist; since it
	# existed before but doesn't exist now, obviously somebody or
	# something removed it while we were waiting for the lock.  Since we
	# would possibly have removed it anyway, it no longer being around
	# is not a reason to print an error message.
	zfs get -Hp -o value name "$1" 2>/dev/null >/dev/null || exit 0

	# Otherwise, if the snapshot is still there but zfs get still
	# returned an error, the admin should know.
	die "FATAL: couldn't read ${PROPPREFIX}:expires property of \"$1\""
}

if [[ $EPOCHSECONDS -lt "$expires" ]]; then # Hasn't expired yet
	log info "$1 only expires at $(strftime %F\ %H:%M:%S $expires); not removing"
	exit 0
fi
if [[ "$expires" = never ]]; then
	log info "$1 is set to never expire; not removing"
	exit 0
fi

# We don't know if we're about to destroy the snapshot of the latest successful backup, so let's find out.
origin=${1/@*}
zfs list -H -d 1 -t snapshot -S creation -o ${PROPPREFIX}:successful,name $origin \
	| grep '^true' \
	| head -n 1 \
	| read true latestsuccessful

[[ "$1" = "$latestsuccessful" ]] && {
	log warning "not removing \"$1\" as it's the snapshot of the latest (or only) successful backup."
	exit 0
}

# TODO: don't expire snapshots of unsuccessful backups if there are no successful ones?
log info "destroying expired snapshot $1 (expired at $(strftime %F\ %H:%M:%S $expires))"
destroyerror=$(zfs destroy -d "$1" 2>&1) || {
	# Again, exit silently if the snapshot doesn't exist; since it
	# existed before but doesn't exist now, obviously somebody or
	# something removed it while we were waiting for the lock.  Since we
	# would just have removed it anyway, it no longer being around
	# is not a reason to print an error message.
	zfs get -Hp -o value name "$1" 2>/dev/null >/dev/null || exit 0

	# If the snapshot is still there, though, but we failed to destroy
	# it, do print an error:
	die "failed to destroy snapshot $1 (expired at $(strftime %F\ %H:%M:%S $expires)): $destroyerror"
}

exit 0
