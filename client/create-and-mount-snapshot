#!/bin/zsh
#
# This is a zfsbackup helper script that should be started from a zfsbackup
# source config directory as a pre-client script (umount-and-destroy-snapshot,
# which is currently a symlink to this same script, should then be used as
# a post-client script to clean up afterwards).
#
# This script creates a snapshot of the filesystem to be backed up and mounts
# it; or, if called with a numeric argument, it assumes it's been called as a
# post-client script and unmounts/removes the snapshots it created. Eventually
# it will support the following cases:
#
# 1. zfs instance.
#
#    Filesystem name in "zfs-dataset" file.
#
#    The snapshot will be called zpool/path/to/zfs/instance@<snapname>.
#
#    <snapname> defaults to zfsbackup-${BACKUPSERVER}-$EPOCHSECONDS if
#    $BACKUPSERVER is not empty, and simply to 'zfsbackup-$EPOCHSECONDS' if it
#    is. TODO: make this overridable. Problem with overriding: will require manual
#    cleanup of previous snapshots, if any.
#
#    Strictly speaking, the snapshot name doesn't need to contain anything like
#    the date; only one ever needs to exist per filesystem and backup server.
#    The benefit to having two: we can check cheaply whether they're
#    identical, and if yes, just perform a pro forma backup to bring into being
#    a new snapshot on the server. We thus include the epoch in the snapshot name,
#    allowing several snapshots of the same fs to exist.
#
#    If the snapshot to be created already exists (because the script is called
#    again within the same second), it appends a numeric suffix to the snapshot
#    name.
#    
#    The "path" symlink will be manipulated to point to the snapshot.
#
#    If the origin filesystem is mounted, the snapshot will be auto-mounted
#    (except for zfs versions affected by https://github.com/openzfs/zfs/issues/9381).
#    If the origin filesystem is not mounted (and when using a buggy zfs version),
#    the script uses mount(8) to mount the snapshot explicitly in a temporary
#    directory. Disadvantage: if the post-client script is not run, the
#    temporary directory and mount stick around forever.
#
# 2. zfs subtree.
#
#    Name of topmost filesystem in tree is in the "zfs-dataset-root" file;
#    "zfs-dataset" contains the name of the filesystem to be actually backed up.
#    The "recursive-snapshot" flag file exists.
#    "no-xdev" should exist, otherwise rsync won't traverse the child
#    filesystems.
#
#    Optionally, "no-snapshot" can contain a list of (child) filesystems that
#    should be excluded from the backup. This is implemented by snapshotting
#    them as well initally ("zfs snapshot -r" doesn't make exceptions), but
#    removing the snapshots immediately afterwards. Snapshots of any child
#    objects that are not filesystems (but, for example, zvols), as well
#    as of filesystems not mounted somewhere under the mountpoint of the
#    topmost filesystem, are also removed and not included in the backup.
#
#    The snapshot will be called zpool/path/to/zfs/instance@<snapname>.
#
#    <snapname> is generated as above, except that it will include a base36
#    representation of the CRC32 of the name of the topmost dataset; this
#    ensures that overlapping recursive backups don't interfere with each
#    other and each job only removes snapshots it is responsible for.
#
#    "path" should be a directory; the topmost snapshot will be mounted
#    in it, with child snapshots being mounted in appropriate places under
#    it (using regular `mount -t zfs`, not automounts as this is more
#    universal -- it also works on snapshots of filesystems that are not
#    themselves mounted).
#
#    The distinction between zfs-dataset-root and zfs-dataset makes sense if
#    the root just exists for administrative purposes but isn't mounted; e.g.
#    you have something like rpool, rpool/ROOT, rpool/ROOT/debian-1, and
#    rpool/var; and want to recursively snapshot rpool, but have
#    rpool/ROOT/debian-1 be the topmost directory that's backed up.
#
#    subsources.d is supported: the snapshots of child filesystems will be
#    mounted under path/ in the appropriate subsource.d directory. Child
#    filesystems with no sub-source will be excluded from the backup, and
#    their snapshots removed immediately.
#
# 3. zvol with arbitrary mountable filesystem. (TBD -- the code that's there
#    is currently just a placeholder)
#
#    Volume name in "zvol" file.
#
#    The snapshot will be called zpool/path/to/zvol@<snapname>. In order to
#    ensure snapshot visibility without having to set snapdev=visible (which
#    may be undesirable if there are many snapshots), the snapshot will be
#    immediately cloned. We'll mount the clone, not the snapshot itself.
#    This also helps with journaling filesystems that need to write to the
#    volume on mount.
#
#    <snapname> is generated as above.
#
#    The clone will be called zpool/<prefix>_<originalvolumename>_<snapname>.
#
# 4. LVM block device with arbitrary mountable filesystem. (TBD)
#
#    Volume pointed to by the "logicalvolume" symlink (it must point to
#    /dev/vgname/lvname, not to /dev/mapper/vgname--lvname).
#
#    The size of the snapshot will be 100M but this can be overridden using
#    the "snapsize" file.
#
#    The snapshot will be called <prefix>_<originalvolumename>_<snapname>.
#    <prefix> defaults to "snap" and can be overridden using the "snapprefix"
#    file (which can also be empty).
#
#    <snapname> defaults to zfsbackup-${BACKUPSERVER} if $BACKUPSERVER is not
#    empty, and simply to 'zfsbackup' if it is. TODO: make this overridable.
#
#    TODO: support checking for changes between successive snapshots? Not sure
#    it's worth it, because LVM snapshots are expensive.
#
# In the last two cases, the snapshot will be mounted under the directory
# "path" points to using '-o ro,noexec,nosuid,nodev' (also nouuid if the fs
# is xfs).
#
# The default option set will inculde "acl" unless "no-acls" exists and
# "user_xattr" unless "no-xattrs" exists.
#
# The mount options can be overridden using the "snapmountoptions" config file
# (it should contain the full option string, e.g. "ro,noexec").
# 
# TODO: make sure exit status values are consistent and documented.

LOG_LEVEL=${LOG_LEVEL:-debug}
LVM_DEFAULT_SNAPSHOT_SIZE=100M
LVM_DEFAULT_SNAPSHOT_PREFIX=snap
LVM_DEFAULT_SNAPSHOT_SUFFIX=zfsbackup${BACKUPSERVER:+-$BACKUPSERVER}
ZFS_DEFAULT_SNAPSHOT_NAME_PREFIX=zfsbackup${BACKUPSERVER:+-$BACKUPSERVER}
USE_SYSLOG=1
DEFAULT_FUNCTIONS=/usr/local/share/zfsbackup/functions.zsh

. $DEFAULT_FUNCTIONS
[[ -r /etc/zfsbackup/client.conf ]] && . /etc/zfsbackup/client.conf
[[ -r /etc/zfsbackup/create-and-mount-snapshot.conf ]] && . /etc/zfsbackup/create-and-mount-snapshot.conf
[[ -n $USER_FUNCTIONS ]] && [[ -r $USER_FUNCTIONS ]] && . $USER_FUNCTIONS	# allow user functions to override default functions

me="${0:t}:$(pwd)"

function lvm_snapshot() {
# TODO: test -- completely untested
	local ret=0
	[[ -d ./path/. ]] || die "$(pwd)/path is not a directory; aborting."
	if [[ -L logicalvolume ]]; then
		logicalvolume=$(readlink logicalvolume)
	elif [[ -f logicalvolume ]]; then
		logicalvolume="$(<logicalvolume)"
	fi
	vgname="${logicalvolume:h:t}"
	snapparentpath="${logicalvolume:h}"
	[[ -r snapsize ]] && snapsize="$(<snapsize)"; snapsize=${snapsize:-${LVM_DEFAULT_SNAPSHOT_SIZE:-100M}}
	if [[ -r snapprefix ]]; then snapprefix="$(<snapprefix)"; else snapprefix="$LVM_DEFAULT_SNAPSHOT_PREFIX"; fi
	if [[ -r snapmountoptions ]]; then
		snapmountoptions="$(<snapmountoptions)"
	else
		snapmountoptions="ro,noexec,nosuid,nodev"
		[[ -e "no-acls" ]] || snapmountoptions="$snapmountoptions,acl"
		[[ -e "no-xattrs" ]] || snapmountoptions="$snapmountoptions,user_xattr"
	fi
	suffix=$LVM_DEFAULT_SNAPSHOT_SUFFIX
	snapname="${snapprefix:+${snapprefix}_}${logicalvolume}${suffix:+_${suffix}}"
	[[ $snapname = $logicalvolume ]] && die "The name of the snapshot must be different from the name of the volume. Adjust snapprefix and/or LVM_DEFAULT_SNAPSHOT_PREFIX and/or LVM_DEFAULT_SNAPSHOT_SUFFIX."
	if mountpoint -q path; then
		umount -R path || die "Can't umount $(pwd)/path."
	fi
	if [[ -b $snapparentpath/$snapname ]]; then
		((postclient)) || log info "Previous snapshot exists; attempting to remove."
		lvremove --force $snapparentpath/$snapname
		ret=$?
		if [[ $ret -gt 0 ]] && ! ((postclient)); then
			die "lvremove $snapparentpath/$snapname returned an error. Can't continue."
		fi
	fi
	((postclient)) && return $ret	
	lvcreate -s -L "$snapsize" -n "$snapname" "$logicalvolume"
	ret_lvcreate=$?
	if ! [[ "$ret_lvcreate" = 0 ]]; then
		die 'lvcreate -s -L "$snapsize" -n "$snapname" "$logicalvolume" returned an error  ($ret_lvcreate).'
	fi
	if ! [[ -r snapmountoptions ]]; then
		fstype="$(findmnt -n -o FSTYPE "$blockdev")"
		[[ "$fstype" = xfs ]] && snapmountoptions="$snapmountoptions,nouuid"
	fi
	mount "$snapparentpath/$snapname" "$(pwd)/path" -o "$snapmountoptions"
	ret_mount=$?
	[[ "$ret_mount" = 0 ]] && return 0
	log crit "'mount \"$snapparentpath/$snapname\" \"$(pwd)/path\" -o \"$snapmountoptions\"' failed with status $ret_mount."
	log notice "Mounting failed; attempting to remove '$blockdev'."
	lvremove --force "$snapparentpath/$snapname"
	return $ret_mount
}

function zvol_snapshot() {
# TODO this is just copypasted old code, needs review/rewrite TODO
	die "zvol snapshot creation and mounting is not currently implemented"
	abort_unless_path_exists # doesn't return on error
	zvol="$(<zvol)"
	zpool="${zvol/\/*/}"
#	initvars
	snapparentpath="/dev/$zpool"
	abortifprevsnapexists	# TODO: test existence of both snapshot and clone
	snapname="$zvol@$snapname"
	clonename="${clonerefix:+${cloneprefix}_}${zvol}_${snapname}"
	zfs snapshot "$snapname"
	ret_snapshot=$?
	if [[ "$ret_snapshot" = 0 ]]; then
		zfs clone "$snapname" "$zpool/$clonename"
		ret_clone=$?
		if [[ "$ret_clone" = 0 ]]; then
			echo "$clonename" >snapname	# we must write the clonename here because abortifprevnsapexists() wouldn't necessarily see the snap device under /dev (e.g. due to snapdev=hidden)
			if try_mount "$snapparentpath/$clonename"; then
				exit 0
			else
				zfs_destroy "$zpool/$clonename"
				zfs_destroy "$snapname"
				exit 3
			fi
		else
			echo "$0: FATAL: 'zfs clone \"$snapname\" \"$zpool/$clonename\"' failed with status $ret_clone. Will try to destroy $snapname and abort." >&2
			zfs_destroy "$snapname"
			exit 5
		fi
	else
		echo "$0: FATAL: 'zfs snapshot \"$snapname\"' failed with status $ret_snapshot. Aborting." >&2
		exit 4
	fi
}

# umount a filesystem, even if it's mounted in more than one place. Argument: filesystem name (not mountpoint).
function umount_fs() {
	local mp="" ret
	mp="$(findmnt -n -o TARGET $1)"	# Is the fs mounted?
	while [[ -n "$mp" ]]; do
		log debug "umount_fs(): $1 is mounted; unmounting"
		umount $1
		ret=$?
		((ret)) && break	# if we're unable to unmount the fs, e.g. because it's busy, this would be a potentially endless loop, so break out of it
		mp="$(findmnt -n -o TARGET $1)"	# Is the snapshot *still* mounted? (It might have been mounted in more than one location.)
	done
	return ret
}

# Helper function to umount and destroy a snapshot; argument is the snapshot (or clone) name; should only be called on objects zfsbackup created,
# because it will happily destroy anything (modulo the sanity check below). Returns with the exit code of `zfs destroy`.
function zfs_destroy() {	 
	local objecttype=$(zfs get type -H -o value $1)
	local snap_mp
	case $objecttype in
		snapshot)		: ;;
		filesystem|volume)	if [[ $(zfs get origin -H -o value $1) = - ]]; then
						log alert "zfs_destroy() called on '$1', which appears to be a regular $objecttype, not a clone or snapshot! Not destroying."
						return 111
					fi;;
		*)			log err "zfs_destroy() called on '$1', of unexpected type '$objecttype'. Not destroying it."
					return 1;;
	esac
	umount_fs $1
	zfs destroy $1	# try to destroy it regardless of whether umount succeeded; at worst, zfs destroy will also fail
}

# Helper function to destroy all snapshots of an object that this script created (detected by their names matching the prefix this script uses),
# except the one that was used to create the last successful backup. By keeping that one, we can save time and i/o on the next backup run if
# the filesystem is not changed between backup runs.
function destroy_outdated_snapshots() { # arg1=dataset whose snapshots to destroy; arg2=prefix of snapshot name to match; arg3=name of last succesful snapshot (optional)
	local ret=0 final_ret=0 snap msg snap_mp
	local zfsdataset=$1
	local prefix=$2
	local last_successful=$3
	[[ -z $last_successful ]] && [[ -f last-successfully-backed-up-snapshot-name ]] && last_successful=$(<last-successfully-backed-up-snapshot-name)
	zfs list -t snap -d 1 $zfsdataset -Hp -o name -s creation | fgrep $zfsdataset@$prefix | while read snap; do
		umount_fs $snap	# we unmount it even if we don't destroy it
		[[ $snap = $last_successful ]] && continue	# leave this one alone; it's useful during the next backup run, to see if there were changes
		log debug "destroying $snap."
		msg=$(zfs_destroy $snap 2>&1)
		ret=$?
		((final_ret+=ret))
		((ret)) && log warning "failed to destroy '$snap': $msg ($ret). Please look into why this happened."
	done
	return final_ret
}

snap_mp="" # this needs to be global as mount_zfs_snapshot() returns a value in it
# Helper function. Mounts a snapshot and sets the global snap_mp variable to
# path of mountpoint. If no mountpoint specified, optinally creates tempdir
# (depending on $mount_method) and symlinks ./path to mountpoint.
function mount_zfs_snapshot() {	# usage: mount_zfs_snapshot fsname snapname fsmountpoint [ snapmountpoint ]
	local zfsdataset=$1
	local snapname=$2
	local zfs_mp=$3
	snap_mp=$4
	local ret mountmsg
	if [[ -z $snap_mp ]]; then	# snap_mp not passed in; we were called by zfs_snapshot(), not recursive_zfs_snapshot()
		if [[ $mount_method = cd ]]; then
			for i in {1..5}; do # This needs to be retried a few times because of some timing issue on some kernels
				# switch to the directory to force it to become mounted
				if pushd "$zfs_mp/.zfs/snapshot/$snapname/." && popd; then
					break
				else
					sleep 0.5
				fi
			done
		fi
		snap_mp="$(findmnt -n -o TARGET $zfsdataset@$snapname)"	# Did the mount succeed? Is the snapshot mounted now?
		if [[ -z "$snap_mp" ]]; then	# we need to workaround "Object is remote" or "Too many levels of symbolic links" automount bug, or use mount(8) for some other reason
			tempdir=$(mktemp -d) || { log emerg "Failed to create temporary directory."; return 111 }
			mountmsg=$(mount -t zfs $zfsdataset@$snapname $tempdir 2>&1)
			ret=$?
			if ((ret)); then
				rmdir $tempdir
				log crit "Failed to mount $zfsdataset@$snapname on $tempdir."
				return $ret
			fi
			snap_mp=$tempdir
		fi
		if [[ -d "$snap_mp/." ]] && mountpoint -q "$snap_mp/."; then
			if [[ -e path ]]; then
				rm -f path || { log emerg "Failed to remove old 'path' symlink from $(pwd)."; return 111 }
			fi
			ln -sf "$snap_mp" path || { log emerg "Failed to make '$(pwd)/path' a symlink to '$mp/.zfs/snapshot/$snapame'."; return 111 }
		else
			log emerg "I expected $zfsdataset@$snapname to be mounted in $snap_mp, but it isn't. Maybe we lost a race with a snapshot remove operation?"
			return 111
		fi
	else	# snap_mp specified; we were called by recursive_zfs_snapshot()
		[[ -e $snap_mp ]] || mkdir -p $snap_mp	# try to automatically handle potential trivial problem where the mountpoint doesn't exist at all
		mountmsg=$(mount -t zfs $zfsdataset@$snapname $snap_mp 2>&1)
		ret=$?
		if ((ret)); then
			log emerg "Unable to mount $zfsdataset@$snapname under $snap_mp: $mountmsg ($ret)."
		fi
		return $ret
	fi
	return 0
}

function zfs_snapshot_postclient() {
	[[ -e path ]] || return 0	# If the -e test fails, path is either a symlink to a directory that doesn't exist (so it can't be a mountpoint), or it doesn't exist at all; in both cases, there is nothing for this function to do so it can return early.
	local zfsdataset="$(<zfs-dataset)" msg ret final_ret
	[[ -L path ]] || die "$(pwd)/path is not a symlink."
	local source=$(findmnt -n -o source $(readlink -f path) 2>/dev/null)
	if [[ -n "$source" ]]; then	# We use findmnt instead of mountpoint -q because the latter will cause an automount attempt if path is a symlink that points to a zfs snapshot directory
		if [[ "$source" =~ ^[^/].*@. ]]; then	# check to see if what's mounted is a zfs snapshot
			log debug "$(readlink -f path/.) is still mounted. Trying to umount."
			msg=$(umount path >&2)
			ret=$?
			((ret)) && log err "Failed to umount $(readlink -f path/.): $msg ($ret). Continuing regardless."
			((final_ret+=$ret))
		else
			die "$source, which is mounted on $(readlink -f path/.), does not appear to be a zfs snapshot. This should not happen; aborting."
		fi
	fi
	# If mount_method=mount (and not 'cd'), it means we created a temporary directory and mounted the snapshot in it explicitly. We can remove that temporary directory now.
	[[ $mount_method = mount ]] && rmdir $(readlink -f path)
	destroy_outdated_snapshots $zfsdataset $ZFS_DEFAULT_SNAPSHOT_NAME_PREFIX
	((final_ret+=$?))
	return $final_ret
}

function zfs_snapshot() {
# TODO: test
	local ret=0 snap tempdir snap_mp snapname i=0 msg
	local prefix=$ZFS_DEFAULT_SNAPSHOT_NAME_PREFIX
	local zfsdataset="$(<zfs-dataset)"
	[[ -e no-xdev ]] && log warning "no-xdev exists and you're requesting a non-recursive snapshot. This is probably not what you want. no-xdev will have no effect."
	# If post-client wasn't run, it's possible the last snapshot is still mounted. Umount it now, if we can, and perform all the other cleanup if necessary.
	zfs_snapshot_postclient
	# If our dataset is not mounted, we can't automount the snapshot and have to mount it explicitly. Automounting is better because then we don't leave the mount around indefinitely if the post-client is not called.
	if [[ "$(zfs get -Hp -o value mounted "$zfsdataset")" = no ]]; then
		mount_method=mount
		log info "$zfsdataset is not mounted; we have to mount the snapshot via mount(8)."
	else
		# ditto: prefer automounting, if possible.
		zfs_mp="$(zfs get -Hp -o value mountpoint "$zfsdataset")"
		[[ "$zfs_mp" = legacy ]] && mount_method=mount && log info "$zfsdataset has a legacy mountpoint; we have to mount the snapshot via mount(8)."
		[[ "$zfs_mp" = none ]] && mount_method=mount && log info "$zfsdataset has no configured mountpoint; we have to mount the snapshot via mount(8)."
	fi
	snapname="$prefix-${EPOCHSECONDS}"
	# If we're being run within one second of the last backup (as unlikely as that may be), the previous snapshot can still exist, and we need to come up with a new name for our snapshot.
	while zfs get -H -o value type "$zfsdataset@$snapname" >/dev/null 2>/dev/null; do
		log debug "$zfsdataset@$snapname exists; trying a different snapshot name."
		snapname="$prefix-${EPOCHSECONDS}.$((++i))"
	done
	zfs snapshot "$zfsdataset@$snapname"	# maybe set some properties? what would make sense?
	ret_snapshot=$?
	if [[ "$ret_snapshot" = 0 ]]; then	# snapshot succeeded, mount it
		mount_zfs_snapshot $zfsdataset $snapname $zfs_mp || exit 111 # mount_zfs_snapshot already logged the error; we merely need to quit here
	else
		die "'zfs snapshot \"$zfsdataset@$snapname\"' failed with status $ret_snapshot. Aborting."
	fi
}

# these variables are global because recursive_zfs_snapshot_init() sets them for other functions to use
zfsdataset=""
prefix=""
newline='
'	# we'll use this as IFS
OLDIFS="$IFS"
typeset -U zfs_instances	# will hold a recursive list of all child filesystems of the tree rooted at $zfsdataset
typeset -U all_children		# will hold a recursive list of all child objects of the tree rooted at $zfsdataset; anything that's not a filesystem automatically goes into nosnapshot
typeset -U nosnapshot		# will hold a list of filesystems we aren't supposed to snapshot and back up. Of course, zfs snapshot -r will snapshot them anyway, but we immediately remove theri snapshots.
typeset -A subsource_dirs	# will hold zfs-dataset->subsource_dir pairs; will be used to mount just-created snapshots under the appropriate subsource_dir/path/
typeset -i 36 crc32	# we'll print this number in base36 to make it as short as possible
typeset -A zfs_mp	# hash. key: zfs dataset name; value: mounpoint of dataset.

function recursive_zfs_snapshot_init() { # bits common to preclient and postclient
	local name type f subsource_dir subsource_ds
	local -U subsource_datasets
	[[ -e path ]] || mkdir path	# Automatically handle oversight where path/ doesn't exist at all; only bail on more complicated errors, such as when it exists as a file
	[[ -L path ]] && die "$(pwd)/path is not a directory."
	[[ -d path ]] || die "$(pwd)/path is not a directory."
	if [[ -e zfs-dataset-root ]]; then
		zfsdataset="$(<zfs-dataset-root)"
	else
		zfsdataset="$(<zfs-dataset)"
	fi
	echo -n "$zfsdataset" | cksum | read crc32 unused
	prefix="$ZFS_DEFAULT_SNAPSHOT_NAME_PREFIX-${crc32##36\#}"	# The suffix includes the CRC32 of the origin fs, so that when we remove snapshots recursively, we only remove ones created as a result of this backup job, not one ones created by a recursive backup of a subtree or a higher level fs. CRC32 isn't a strong hash, but it should be good enough for this purpose.
	# Also: if pool/fs1 mounted on /fs1 and pool/fs1/fs2 mounted on /fs2 -- since /fs2 is not under /fs1, we won't back it up; however, we also don't want to destroy any of its snapshots.
	IFS="$newline	"
	[[ -r no-snapshot ]] && nosnapshot=($(<no-snapshot))	# a list of filesystems that should not be included in the recursive snapshot
	if [[ subsources.d ]] && ! [[ -e no-xdev ]]; then	# If subsources.d exists and no-xdev does not, we assume that only the filesystems enumerated therein need to be backed up, and remove snapshots of all others.
		IFS="$newline"
		find $(pwd)/. -xdev -name zfs-dataset | while read f; do
			subsource_dir=${f:h}
			if [[ -e $subsource_dir/url ]]; then	# Try to avoid processing random files called zfs-dataset that we may find by chance; only process ones that have a corresponding 'url' file.
				subsource_ds=$(<$f)
				subsource_datasets=($subsource_datasets $subsource_ds)
				subsource_dirs[$subsource_ds]=$subsource_dir
			fi
		done
		IFS="$OLDIFS"
	fi
	zfs list -r -H -o name,type $zfsdataset | while read name type; do
		all_children=($all_children $name)
		if [[ $type = filesystem ]] && { [[ $#subsource_datasets = 0 ]] || [[ ${subsource_datasets[(I)$name]} -gt 0 ]] }; then	# If subsource_datasets is not empty, we only keep snapshots of datasets that occur in it
			zfs_instances=($zfs_instances $name)
		else
			if ! [[ $type = filesystem ]]; then
				log debug "Skipping $name of type $type (not a filesystem)."
			else
				log debug "Skipping $name because it has no subsources.d directory under $(pwd)."
			fi
			nosnapshot=($nosnapshot $name)
		fi
	done
	IFS="$OLDIFS"
	# populate nosnapshot with other filesystems we don't want to include in the backup
	for i in $zfs_instances[@]; do
		zfs_mp[$i]="$(zfs get -Hp -o value mountpoint "$i")"
		if [[ "$zfs_mp[$i]" = legacy ]]; then
			# if the entire zfs subtree has legacy mountpoints, with could assume the zfs hierarchy reflects a mount hierarchy and back them up like that regardless, but I expect that to be a very rare case
			log info "Filesystems with legacy mountpoints are not supported for recursive snapshots; we won't include $i in the backup."	
			nosnapshot=($nosnapshot[@] $i)
		else
			canmount="$(zfs get -Hp -o value canmount "$i")"
			if [[ $canmount = off ]]; then
				# if the origin fs can't be mounted, then its snapshot shouldn't be either
				log debug "Skipping $i with canmount=off"
				nosnapshot=($nosnapshot[@] $i)
			fi
		fi
		[[ -z "$zfs_topmost_mp" ]] && zfs_topmost_mp=$zfs_mp
		if [[ $zfs_mp[$i] = ${zfs_mp[$i]##$zfs_topmost_mp} ]]; then	# We try to cut off "zfs_topmost_mp" from the beginning of zfs_mp; if the result is the unchanged zfs_mp, then the mountpoint of this fs is not under our topmost mountpoint and we skip it.
			log info "$i is mounted unter '$zfs_mp[$i]', which is not under '$zfs_topmost_mp'; we won't include $i in the backup."
			nosnapshot=($nosnapshot[@] $i)
		fi
	done
}

function recursive_zfs_snapshot_postclient() {
	local i last_successful msg ret=0 final_ret=0
	[[ -n "$zfsdataset" ]] || recursive_zfs_snapshot_init # sets $prefix, $zfsdataset, $nosnapshot, $zfs_instances, ...
	if [[ -n $(findmnt -n -o source $(readlink -f path) 2>/dev/null) ]]; then	# It's impossible we're running concurrently with another backup job because zfsbackup-client enforces mutual exclusion; thus this is safe. We use findmnt instead of mountpoint -q because the latter will cause an automount attempt if path is a symlink that points to a zfs snapshot directory
		# Technically, it's possible that path itself isn't a mountpoint, but a subdirectory is -- we don't have to care about that, but (TODO) it would be nice to print a warning
		msg=$(umount -R path >&2)
		ret=$?
		((ret)) && log err "$(readlink -f path) is a mountpoint, and we failed to umount it: $msg ($ret). Continuing regardless."
	fi
	if [[ -z "$last_successful" ]]; then
		[[ -f last-successfully-backed-up-snapshot-name ]] && last_successful=$(<last-successfully-backed-up-snapshot-name)
	fi
	for i in $zfs_instances; do	# recursively delete all zfsbackup snapshots except the last successful one
		destroy_outdated_snapshots $i $prefix ${last_successful/*@/$i@}	# $last_successful includes the dataset name, so always replace it with the name of the dataset we're processing
		ret=$?
		((final_ret+=$ret))
	done
	for i in $nosnapshot; do	# recursively delete all zfsbackup snapshots that may have been left around
		destroy_outdated_snapshots $i $prefix this-cannot-match	# objects in $nosnapshot should not have zfsbackup snapshots; by passing "this-cannot-match" to destroy_outdated_snapshots() we ensure that it will not treat any snapshot it finds as the last successful one (which would be spared)
		ret=$?
		((final_ret+=$ret))
	done
	return $final_ret
}

function recursive_zfs_snapshot() { # In this case, ./path is a directory (not a symlink), and we mount our system of recursive snapshots under it.
# If subsources.d exists and no-xdev does not, we assume that only the filesystems enumerated therein need to be backed up, and remove snapshots of all others.
# TODO: test
	local unused msg snap_exists=1 i=0 z currentmp
	local ret=0
	
	recursive_zfs_snapshot_init # sets $prefix, $zfsdataset, $nosnapshot, $zfs_instances, ...
	recursive_zfs_snapshot_postclient # removes outdated snapshots and such; unmounts path/ recursively -- thus also useful during pre-client
	if ! [[ -e no-xdev ]] && ! [[ -d subsources.d ]]; then
		log warning "You're requesting recursive zfs snapshots but don't have no-xdev in $(pwd), and also no subsources.d/. This appears nonsensical; are you sure it is what you want?"
	elif [[ -e no-xdev ]] && [[ -d subsources.d ]] && ! [[ -e nowarn.recursive.noxdev ]]; then
		log warning "You're requesting recursive zfs snapshots and have BOTH no-xdev and subsources.d/ in $(pwd). This is not technically invalid, but unusual enough (and difficult enough to get right) to warrant a warning. Touch $(pwd)/nowarn.recursive.noxdev to silence it."
	fi

	snapname="$prefix-${EPOCHSECONDS}"
	# If we're being run within one second of the last backup (as unlikely as that may be), the previous snapshot can still exist, and we need to come up with a new name for our snapshot.
	while [[ $snap_exists = 1 ]]; do
		snap_exists=0
		for z in $all_children; do
			if zfs get -H -o value type "$z@$snapname" >/dev/null 2>/dev/null; then
				log debug "$z@$snapname exists; trying a different snapshot name."
				snapname="$prefix-${EPOCHSECONDS}.$((++i))"
				snap_exists=1
			fi
		done
	done
	zfs snapshot -r "$zfsdataset@$snapname"
	ret_snapshot=$?
	if [[ "$ret_snapshot" = 0 ]]; then
		for z in $nosnapshot[@]; do
			zfs_destroy $z@$snapname || log warning "$z isn't supposed to be snapshotted and backed up, but I was unable to destroy $z@$snapname which 'zfs snapshot -r '$zfsdataset@$snapname' created."
		done
		zfs_instances=(${zfs_instances:|nosnapshot})
		for z in $zfs_instances[@]; do
			if [[ -n $subsource_dirs[$z] ]]; then
				currentmp=$subsource_dirs[$z]/path
			else
				currentmp=$PWD/path/${zfs_mp[$z]##$zfs_topmost_mp}
				currentmp=${currentmp/\/\///}	# remove double slash (purely cosmetic)
			fi
			log debug "Trying to get $z@$snapname mounted in $currentmp."
			mount_zfs_snapshot $z $snapname $zfs_mp[$z] $currentmp
			ret=$?
			if ((ret)); then
				log crit "'mount_zfs_snapshot $z $snapname $zfs_mp[$z] $currentmp' returned an error ($ret). Trying to roll back and exiting." 
				recursive_zfs_snapshot_postclient
				die "Failed to recursively mount the snapshots of $zfsdataset. Aborting."
			fi
		done
	else
		die "'zfs snapshot -r '$zfsdataset@$snapname' failed with status $ret_snapshot. Aborting."
	fi
	return 0 # we treated previous errors as fatal, so if we're here, everything is fine
}

zmodload zsh/datetime

postclient=0
[[ "$1" = <-> ]] && postclient=1	# if $1 is a number, we're being called as a post-client script
[[ "${0:t}" = umount-and-destroy-snapshot ]] && postclient=1	# the name is also a strong hint
mount_method=cd
[[ -f /sys/module/zfs/version ]] && grep -q '^0\.8\.[2345]' /sys/module/zfs/version && mount_method=mount	# these versions can't automount some snapshots and report "Object is remote", but mounting them via mount(8) works

if [[ -e logicalvolume ]]; then # this is case #4 explained above.
	lvm_snapshot
elif [[ -r zvol ]]; then # this is case #3
	zvol_snapshot
elif [[ -r recursive-snapshot ]]; then # this is case #2
	((postclient)) && { recursive_zfs_snapshot_postclient; exit $? }
	recursive_zfs_snapshot
elif [[ -r zfs-dataset ]]; then # this is case #1
	((postclient)) && { zfs_snapshot_postclient; exit $? }
	zfs_snapshot
else
	die "Neither of 'logicalvolume', 'zvol' or 'zfs-dataset' exist in $(pwd). I don't know what to do; aborting."
fi
exit $?
